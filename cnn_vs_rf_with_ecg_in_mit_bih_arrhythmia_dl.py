# -*- coding: utf-8 -*-
"""CNN VS RF with ECG in MIT-BIH-arrhythmia-dl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/122eMJuyR3KDh7bv8rbN38yKLA1D7Z6TK

https://github.com/csaguiar/arrhythmia-detection/blob/master/MIT-BIH-arrhythmia-dl.ipynb

# Detecting arrythmia using Deep Leaning

## Loading modules
"""

!pip install wfdb wget

import pandas as pd
import numpy as np
import wfdb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from pathlib import Path

from keras.models import Sequential
from keras.layers import Conv1D, LSTM, Dense, Dropout, TimeDistributed
from keras.optimizers import Adam

import matplotlib.pyplot as plt

"""## Download Dataset"""

import wget
import zipfile

database_filename = "mit-bih-arrhythmia-database-1.0.0.zip"
database_path = Path(database_filename)

if not database_path.exists():
    url = f'https://storage.googleapis.com/mitdb-1.0.0.physionet.org/{database_filename}'
    wget.download(url)
    with zipfile.ZipFile(database_filename, 'r') as zip_ref:
        zip_ref.extractall(".")

"""## Loading data

Loading list of records available, from dataset available [here](https://physionet.org/content/mitdb/1.0.0/)
"""

records = np.loadtxt("mit-bih-arrhythmia-database-1.0.0/RECORDS", dtype=int)

"""Defining invalid beats as well as abnormal beats, according to [Physiobank](https://archive.physionet.org/physiobank/annotations.shtml)"""

invalid_beat = [
    "[", "!", "]", "x", "(", ")", "p", "t", 
    "u", "`", "'", "^", "|", "~", "+", "s", 
    "T", "*", "D", "=", '"', "@"
]

abnormal_beats = [
    "L", "R", "B", "A", "a", "J", "S", "V", 
    "r", "F", "e", "j", "n", "E", "/", "f", "Q", "?"
]

"""## Processing dataset

This function classify a beat according to its symbol and the list provided above.
"""

def classify_beat(symbol):
    if symbol in abnormal_beats:
        return 1
    elif symbol == "N" or symbol == ".":
        return 0

"""Given a signal, the beat location, and the window to be used as a sequence, this function gets the sequence. It will return an empty array in case of an invalid beat or empty sequence."""

def get_sequence(signal, beat_loc, window_sec, fs):
    window_one_side = window_sec * fs
    beat_start = beat_loc - window_one_side
    beat_end = beat_loc + window_one_side
    if beat_end < signal.shape[0]:
        sequence = signal[beat_start:beat_end, 0]
        return sequence.reshape(1, -1, 1)
    else:
        return np.array([])

"""The code below will build a list of labels and sequences as well as map the sequences for each patient. The percentage calculated represents the ratio of abnormal beats in each patient data."""

all_sequences = []
all_labels = []
window_sec = 3
subject_map = []
for subject in records:
    record = wfdb.rdrecord(f'mit-bih-arrhythmia-database-1.0.0/{subject}')
    annotation = wfdb.rdann(f'mit-bih-arrhythmia-database-1.0.0/{subject}', 'atr')
    atr_symbol = annotation.symbol
    atr_sample = annotation.sample
    fs = record.fs
    scaler = StandardScaler()
    signal = scaler.fit_transform(record.p_signal)
    subject_labels = []
    for i, i_sample in enumerate(atr_sample):
        label = classify_beat(atr_symbol[i])
        sequence = get_sequence(signal, i_sample, window_sec, fs)
        if label is not None and sequence.size > 0:
            all_sequences.append(sequence)
            subject_labels.append(label)

    normal_percentage = sum(subject_labels) / len(subject_labels)
    subject_map.append({
        "subject": subject,
        "percentage": normal_percentage,
        "num_seq": len(subject_labels),
        "start": len(all_labels),
        "end": len(all_labels)+len(subject_labels)
    })
    all_labels.extend(subject_labels)

pd.Series(all_labels).describe()

all_sequences[0:3]

for i in [0,1,2]:
  print(all_sequences[i].shape)

"""Creating bins to be used to stratify the train and validation split. """

subject_map = pd.DataFrame(subject_map)

subject_map.head()

"""The code presented will create class in each patient is segmented."""

bins = [0, 0.2, 0.6, 1.0]
subject_map["bin"] = pd.cut(subject_map['percentage'], bins=bins, labels=False, include_lowest=True)
subject_map.head()

subject_map.describe()

"""Now, the dataset is split into train and validation, stratifying by the bin defined above."""

train, validation = train_test_split(subject_map, test_size=0.25, stratify=subject_map["bin"], random_state=42)

"""This function build a dataset based on the map for each split."""

def build_dataset(df, all_sequences, all_labels):
    sequences = []
    labels = []
    for i, row in df.iterrows():
        start = int(row["start"])
        end = int(row["end"])
        sequences.extend(all_sequences[start:end])
        labels.extend(all_labels[start:end])
        
    return np.vstack(sequences), np.vstack(labels)

X_train, y_train = build_dataset(train, all_sequences, all_labels)
X_val, y_val = build_dataset(validation, all_sequences, all_labels)

X_train.shape, y_train.shape

X_train[:, :, 0].shape

"""## Training the model

For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.

For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.

‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty

‘liblinear’ and ‘saga’ also handle L1 penalty

‘saga’ also supports ‘elasticnet’ penalty

‘liblinear’ does not support setting penalty='none'
"""

from sklearn.linear_model import LogisticRegression
import numpy as np
import pandas as pd
from sklearn.metrics import average_precision_score, roc_auc_score
from sklearn.metrics import roc_curve, auc, accuracy_score  # log_loss
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score

def print_stats(preds, labels):
  print("Accuracy = {}".format(accuracy_score(labels, preds)))
  print("Precision = {}".format(precision_score(labels, preds)))
  print("Recall = {}".format(recall_score(labels, preds)))

def gs(X, y, m, param, scoring, cv, n_jobs):
    GridSearch = GridSearchCV(m, param, scoring=scoring, cv=cv, n_jobs=n_jobs)
    GridSearch.fit(X=X, y=y)
    model = GridSearch.best_estimator_
    print('best_score_:', GridSearch.best_score_, GridSearch.best_params_)
    return model

from sklearn.ensemble import RandomForestClassifier 
def classify_RF(y, X, seed=2020, scoring='roc_auc', cv=5, n_jobs=4):
    cols = X.columns
    param = {
        'max_features': ['sqrt', 'log2'],  # 1.0/3 
             'max_depth': range(10, 14, 2)
             # 'criterion' :['gini', 'entropy']
             }
    m = RandomForestClassifier(
        n_estimators=10, random_state=seed, min_samples_split=20, class_weight="balanced")
    model = gs(X, y, m, param, scoring=scoring, cv=cv, n_jobs=n_jobs)
    coef = pd.DataFrame(list(zip(cols, np.round(
        model.feature_importances_, 2))), columns=['Variable', 'coef_'])
    coef['imp'] = coef['coef_'].abs()
    return model, coef.sort_values('imp', ascending=False)

m_, c_ = classify_RF(y_train.ravel(), pd.DataFrame(X_train[:, :, 0]), seed=2020, scoring='accuracy', cv=4, n_jobs=2)
# , c_ 
ytest_pred = m_.predict(X_val[:, :, 0])
print_stats(ytest_pred, y_val)    
m_
# best_score_: 0.9102368067602846 {'max_depth': 8, 'max_features': 'auto'}
# Accuracy = 0.7921855363875243
# Precision = 0.6713230200877396
# Recall = 0.6902077151335312

# best_score_: 0.9108159200629632 {'max_depth': 10}
# Accuracy = 0.8210899120301611
# Precision = 0.7383906869643085
# Recall = 0.6851038575667656

!lscpu

# LogisticRegression
# best_score_: 0.7592099133190755 {'C': 0.1} solver='sag', tol=0.01,
# Accuracy = 0.7579115731749114
# Precision = 0.6668280090351726
# Recall = 0.49056379821958457

"""### CNN model"""

from keras.models import Sequential
from keras.layers import Conv1D, Flatten, Dense, Dropout
from keras.optimizers import Adam

sequence_size = X_train.shape[1]
n_features = 1

cnn_model = Sequential([
    Conv1D(
        filters=8,
        kernel_size=4,
        strides=1,
        input_shape=(sequence_size, n_features),
        padding="same",
        activation="relu"
    ),
    Flatten(),
    Dropout(0.5),
    Dense(
        1,
        activation="sigmoid",
        name="output",
    )
])

optimizer = Adam(lr=0.001)
# Compiling the model
cnn_model.compile(
    optimizer=optimizer,
    loss="binary_crossentropy",
    metrics=["accuracy"]
)
cnn_model.summary()

hist_cnn = cnn_model.fit(
    X_train, 
    y_train, 
    batch_size=128,
    epochs=5,
    validation_data=(X_val, y_val)
)

cnn_model.evaluate(X_val, y_val)

# summarize history for accuracy
plt.plot(hist_cnn.history['accuracy'])
plt.plot(hist_cnn.history['val_accuracy'])
plt.title('CNN model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(hist_cnn.history['loss'])
plt.plot(hist_cnn.history['val_loss'])
plt.title('CNN model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### CNN with LSTM model"""

sequence_size = X_train.shape[1]
n_features = 1 
n_subsequences = 4
subsequence_size = int(sequence_size / n_subsequences)

# Reshaping to be (samples, subsequences, sequence, feature)
X_train = X_train.reshape(-1, n_subsequences, subsequence_size, n_features)
X_val = X_val.reshape(-1, n_subsequences, subsequence_size, n_features)

cnn_lstm_model = Sequential([
    TimeDistributed(
        Conv1D(
            filters=8,
            kernel_size=4,
            strides=1,
            padding="same",
            activation="relu"
        ), 
        input_shape=(n_subsequences, subsequence_size, n_features)
    ),
    TimeDistributed(Flatten()),
    LSTM(units=4),
    Dense(
        1,
        activation="sigmoid",
        name="output",
    )
])

optimizer = Adam(lr=0.001)
# Compiling the model
cnn_lstm_model.compile(
    optimizer=optimizer,
    loss="binary_crossentropy",
    metrics=["accuracy"]
)
cnn_lstm_model.summary()

train_params = {
    "batch_size": 128,
    "epochs": 5,
    "verbose": 1,
    "validation_data": (X_val, y_val),
}

history_cnn_lstm = cnn_lstm_model.fit(X_train, y_train, **train_params)

cnn_lstm_model.evaluate(X_val, y_val)

# summarize history for accuracy
plt.plot(history_cnn_lstm.history['accuracy'])
plt.plot(history_cnn_lstm.history['val_accuracy'])
plt.title('CNN-LSTM model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history_cnn_lstm.history['loss'])
plt.plot(history_cnn_lstm.history['val_loss'])
plt.title('CNN-LSTM model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

